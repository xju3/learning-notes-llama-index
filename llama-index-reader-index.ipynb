{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Global Environment Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index-readers-smart-pdf-loader\n",
    "! pip install llama_index\n",
    "! pip install llmsherpa\n",
    "! pip install llama-index-readers-pdf-marker\n",
    "! pip install llama-index-readers-llama-parse\n",
    "! pip install llama-parse\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-indices-managed-postgresml\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-docstore-postgres\n",
    "! pip install llama-index-storage-docstore-mongodb\n",
    "! pip install llama-index-storage-index-store-mongodb\n",
    "! pip install llama-index-llms-openai\n",
    "! pip install llama-index-extractors-entity\n",
    "! pip install llama-index-extractors-marvin\n",
    "! pip install unstructured\n",
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.01✅.Set Application Runtime Environment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:env:host: http://localhost:11434, mode: mistral:latest\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/tju/Workspace/books/llama-index-practise/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/tju/Workspace/books/llama-index-practise/.venv/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:env:conn: postgresql://llm:llm@192.168.1.3:5432/llm?sslmode=require\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from env import AppConfig\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import Settings\n",
    "load_dotenv(\".env\")\n",
    "mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "pg_uri = os.getenv(\"PG_URI\")\n",
    "config = AppConfig()\n",
    "logger = config.logger\n",
    "llm = config.llm\n",
    "embedding = config.embedding\n",
    "# using ollama\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.02✅.Test if local llm is working```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(llm.complete(\"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```02.01✅.Read Documents 1: Using SimpleDirectoryReader   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.readers.file.base:> [SimpleDirectoryReader] Total files added: 1\n",
      "DEBUG:fsspec.local:open file: /Users/tju/Workspace/books/llama-index-practise/files/2407.21290v1.pdf\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: TrackSorter: A Transformer-based sorting algori...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: variables from a continuous, multi-dimensional ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Figure 2: The detector schematic shows the top ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: last predicted token is the [SEP] token. This t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: a handful of samples. This adversely affects Wo...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: [12] L. Heinrich, T . Golling, M. Kagan, S. Kle...\n",
      "DEBUG:env:Node ID: 65e5de3e-5ac7-41a9-b3df-bf879aec01dc\n",
      "Text: TrackSorter: A Transformer-based sorting algorithm for track\n",
      "finding in High Energy Physics Yash Melkani1 and Xiangyang Ju2\n",
      "1University of California-Berkeley 2Lawrence Berkeley National\n",
      "Laboratory E-mail: yashmelkani02@gmail.com, xju@lbl.gov Abstract.\n",
      "Track finding in particle data is a challenging pattern recognition\n",
      "problem in High Energy Phy...\n",
      "DEBUG:env:Node ID: 357e24f5-17b2-47ce-bd9b-c315f86d5a72\n",
      "Text: variables from a continuous, multi-dimensional space into\n",
      "discrete spaces. Although some information from the continuous space\n",
      "will inevitably be lost during tokenization, this loss may be\n",
      "acceptable as long as it does not compromise the accuracy of the\n",
      "underlying physics. After all, all physics measurements inherently\n",
      "con- tain some level of un...\n",
      "DEBUG:env:Node ID: 460b97cd-53fd-41b3-b8f6-8d4801b67fb3\n",
      "Text: Figure 2: The detector schematic shows the top half of the\n",
      "detector projected on the r-z plane. The z-axis is along the beam\n",
      "direction. train the model, we randomly pair each track with another\n",
      "track and construct a target sentence following the ordering scheme\n",
      "shown in Fig. 1 for each track pair. In total, our training data\n",
      "contains 349k senten...\n",
      "DEBUG:env:Node ID: 97d16d74-3628-4a4d-869c-996c4db435a8\n",
      "Text: last predicted token is the [SEP] token. This termination\n",
      "condition is not ideal when noise space points 1 are presented as in\n",
      "real data. The model performance is evaluated by the tracking\n",
      "reconstruction efficiency , defined as the fraction of particles that\n",
      "are matched to at least one reconstructed track. Reconstructed track\n",
      "candidates are matc...\n",
      "DEBUG:env:Node ID: 88aa6c99-1693-42d2-9359-a339bd64a00d\n",
      "Text: a handful of samples. This adversely affects Word2Vec training\n",
      "of our initial embedding vectors as well as training of the model\n",
      "itself. In smaller scale experiments focusing on the inner barrel\n",
      "detector region, we came up with physics-inspired embedding vectors\n",
      "for detector modules, such as the global coordinates, rotation matrix\n",
      "and pitch comp...\n",
      "DEBUG:env:Node ID: b6ec0728-1e5e-4fcb-93c5-a509c078ba47\n",
      "Text: [12] L. Heinrich, T . Golling, M. Kagan, S. Klein, et al.,Masked\n",
      "Particle Modeling on Sets: Towards Self-Supervised High Energy Physics\n",
      "Foundation Models, arXiv:2401.13537. [13] A. Huang, Y. Melkani, P .\n",
      "Calafiura, A. Lazar, et al.,A Language Model for Particle Tracking, in\n",
      "Connecting The Dots 2023, 2, 2024. arXiv:2402.10239. [14] M. Lewis, Y.\n",
      "L...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import (DocxReader, \n",
    "                                      HWPReader, \n",
    "                                      PDFReader, \n",
    "                                      EpubReader, \n",
    "                                      FlatReader, \n",
    "                                      HTMLTagReader, \n",
    "                                      ImageCaptionReader, \n",
    "                                      ImageReader, \n",
    "                                      ImageVisionLLMReader, \n",
    "                                      IPYNBReader, \n",
    "                                      MarkdownReader, \n",
    "                                      MboxReader, \n",
    "                                      PptxReader, \n",
    "                                      PandasCSVReader, \n",
    "                                      PandasExcelReader,\n",
    "                                      VideoAudioReader, \n",
    "                                      UnstructuredReader, \n",
    "                                      PyMuPDFReader, \n",
    "                                      ImageTabularChartReader, \n",
    "                                      XMLReader, \n",
    "                                      PagedCSVReader, \n",
    "                                      CSVReader, \n",
    "                                      RTFReader,)\n",
    "reader = SimpleDirectoryReader(\"./files\")\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```02.02✅.Read Documents 2: Using LlamaParse```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
    "from llama_index.readers.pdf_marker import PDFMarkerReader\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "parses = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "document = parses.load_data(\"/Users/tju/Downloads/Books/1.pdf\")\n",
    "logger.debug(len(document))\n",
    "\n",
    "\n",
    "# ❌\n",
    "# llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "# pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\"  # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "# pdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n",
    "# documents = pdf_loader.load_data(pdf_url)\n",
    "\n",
    "# ❌\n",
    "# path = Path(\"/Users/tju/Downloads/Books/1.pdf\")\n",
    "# reader = PDFMarkerReader()\n",
    "# document = reader.load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.01✅.Using splitter to separate documents to chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: TrackSorter: A Transformer-based sorting algori...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: variables from a continuous, multi-dimensional ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Figure 2: The detector schematic shows the top ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: last predicted token is the [SEP] token. This t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: a handful of samples. This adversely affects Wo...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: [12] L. Heinrich, T . Golling, M. Kagan, S. Kle...\n",
      "DEBUG:env:6\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import (TokenTextSplitter, \n",
    "                                          MetadataAwareTextSplitter,\n",
    "                                          SentenceSplitter, \n",
    "                                          CodeSplitter)\n",
    "splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \", include_prev_next_rel=True)\n",
    "nodes = splitter.get_nodes_from_documents(documents=docs)\n",
    "logger.debug(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.02✅.Using Parser to separate documents to chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: TrackSorter: A Transformer-based sorting algori...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: variables from a continuous, multi-dimensional ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: Figure 2: The detector schematic shows the top ...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: last predicted token is the [SEP] token. This t...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: a handful of samples. This adversely affects Wo...\n",
      "DEBUG:llama_index.core.node_parser.node_utils:> Adding chunk: [12] L. Heinrich, T . Golling, M. Kagan, S. Kle...\n",
      "DEBUG:env:6\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import(SentenceWindowNodeParser, \n",
    "                                         SemanticDoubleMergingSplitterNodeParser, \n",
    "                                         SimpleFileNodeParser, \n",
    "                                         SemanticSplitterNodeParser,\n",
    "                                         NodeParser, \n",
    "                                         LlamaParseJsonNodeParser,\n",
    "                                         HTMLNodeParser, \n",
    "                                         JSONNodeParser, \n",
    "                                         SimpleNodeParser, \n",
    "                                         MarkdownNodeParser,\n",
    "                                         LangchainNodeParser, \n",
    "                                         HierarchicalNodeParser, \n",
    "                                         MarkdownElementNodeParser,\n",
    "                                         UnstructuredElementNodeParser)\n",
    "# has error message\n",
    "# parser = UnstructuredElementNodeParser()\n",
    "parser = SimpleFileNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents=docs)\n",
    "logger.debug(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.03✅.Using pipeline to seperate documents to chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Removing unpickleable private attribute _chunking_tokenizer_fn\n",
      "WARNING:root:Removing unpickleable private attribute _split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _sub_sentence_split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _chunking_tokenizer_fn\n",
      "WARNING:root:Removing unpickleable private attribute _split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _sub_sentence_split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _chunking_tokenizer_fn\n",
      "WARNING:root:Removing unpickleable private attribute _split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _sub_sentence_split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _chunking_tokenizer_fn\n",
      "WARNING:root:Removing unpickleable private attribute _split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _sub_sentence_split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _chunking_tokenizer_fn\n",
      "WARNING:root:Removing unpickleable private attribute _split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _sub_sentence_split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _chunking_tokenizer_fn\n",
      "WARNING:root:Removing unpickleable private attribute _split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _sub_sentence_split_fns\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n",
      "WARNING:root:Removing unpickleable private attribute _client\n",
      "WARNING:root:Removing unpickleable private attribute _async_client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.85s/it]\n",
      "100%|██████████| 1/1 [00:39<00:00, 39.93s/it]\n",
      "100%|██████████| 1/1 [00:42<00:00, 42.65s/it]\n",
      "100%|██████████| 1/1 [00:47<00:00, 47.70s/it]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.90s/it]\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:env:nodes: 6\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "title_extractor = TitleExtractor();\n",
    "transformations = [sentence_splitter, title_extractor, embedding]\n",
    "pipeline = IngestionPipeline( transformations = transformations)\n",
    "nodes = pipeline.run(documents=docs, num_workers=4)\n",
    "logger.debug(f\"nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.01✅.Extract meta-data 1: TitleExtractor  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Fri, 22 Nov 2024 16:55:41 GMT'), (b'Content-Length', b'1216')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Fri, 22 Nov 2024 16:55:47 GMT'), (b'Content-Length', b'1069')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Fri, 22 Nov 2024 16:55:59 GMT'), (b'Content-Length', b'1353')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Fri, 22 Nov 2024 16:56:03 GMT'), (b'Content-Length', b'757')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:env:{'document_title': ' \"Transformer-based Track Finding in High Energy Physics: A Novel Approach using Tokenization and Sorting with TRACK SORTER - Overcoming Challenges of Point Cloud Data Processing through LLM Capabilities and Efficient Algorithm Implementation\"\\n\\nThis title reflects the main entities, themes, and contributions of the paper by providing a comprehensive description. It includes:\\n\\n1. The use of transformer-based models for track finding in High Energy Physics (HEP).\\n2. Introducing TRACK SORTER as a novel approach to handle this problem using tokenization and sorting techniques.\\n3. Detailing the importance of converting space points into discrete token ids (tokenization) and addressing challenges associated with handling out-of-vocabulary words in HEP.\\n4. Describing how Large Language Models (LLMs) can be utilized to improve the overall performance of track finding.\\n5. Demonstrating the application and effectiveness of the proposed method through experimentation on the TrackML dataset.'}\n",
      "DEBUG:env:{'document_title': ' \"Formulating Jet Physics Track Finding as Sequence-to-Sequence Problem: A Comparative Study of Binning vs. Vector-Quantization Variational Autoencoder Tokenization and Bart Model Utilization using Omnijet and TrackML Dataset\"\\n\\nThis title accurately reflects the content of the document, including the application of seq2seq models to the track finding problem in jet physics, comparison of tokenizing schemes (binning vs. VQ-VAE), use of the Omnijet framework and TrackML dataset, and implementation of a Bart model for encoding and decoding sequences.'}\n",
      "DEBUG:env:{'document_title': ' \"Development and Training of a Sequence Detector Model Using Transformer Architecture: Data Processing Techniques, Sequence-to-Sequence Encoder-Decoder Design, and Greedy Inference Algorithm\"\\n\\nThis title captures the essence of the document by including the development and training process of a sequence detector model, the specific use of transformer architecture with encoder-decoder structure, the data processing techniques employed (random pairing and constructing target sentences), and the inference process using the greedy search algorithm.'}\n",
      "DEBUG:env:{'document_title': ' \"Performance Evaluation of a Particle Tracking Model: Assessing the Impact of Track Length, Transverse Momentum, and Noise Space Points on Efficiency with Comparative Analysis of Distributions in Dataset\"\\n\\nExplanation: This title encapsulates all the key entities discussed in the document. It specifies that the focus is on performance evaluation of a particle tracking model, which includes the impact of track length and transverse momentum on efficiency, as well as the effect of noise space points on the model\\'s performance. Furthermore, it highlights the importance of comparing distributions in the dataset to understand the differences between testing and potentially larger or more comprehensive datasets.'}\n",
      "DEBUG:env:{'document_title': ' \"A Sequence to Sequence Approach using Transformer-based Language Models for Efficient Particle Tracking in High-Energy Physics: Algorithm Development and Implementation\"\\n\\nExplanation: The title provides a clear description of the content of the paper. It starts by highlighting the problem of particle tracking as a sequence to sequence problem, and the proposed solution using Transformer-based language models. The title also emphasizes the application of this approach in high-energy physics research, and the efficiency gains for low-pT particles. Additionally, it mentions that the study involved the utilization of resources from the National Energy Research Scientific Computing Center (NERSC), and provides information on the availability of data and code.'}\n",
      "DEBUG:env:{'document_title': ' \"Recent Advances in Language Modeling and Machine Learning: A Review of Applications in Particle Physics and Natural Language Processing\"\\n\\nThis title captures the essence of the document, which covers recent research and developments in language modeling and machine learning with a focus on their applications in both particle physics and natural language processing. The title also acknowledges that the content will be a review of these advancements.'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor, \n",
    "    SummaryExtractor, \n",
    "    KeywordExtractor,\n",
    "    PydanticProgramExtractor,\n",
    "    QuestionsAnsweredExtractor,)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "title_extractor = TitleExtractor()\n",
    "title_meta_list = title_extractor.extract(nodes)\n",
    "\n",
    "for item in title_meta_list:\n",
    "    logger.debug(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.02✅.Extract meta-data 2: KeywordExtractor```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "keyword_extractor = TitleExtractor()\n",
    "keyword_meta_list = keyword_extractor.extract(nodes)\n",
    "for item in keyword_meta_list:\n",
    "    logger.debug(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.03❌.Extract meta-data : it needs huggingface api key.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.extractors.marvin import MarvinMetadataExtractor\n",
    "\n",
    "entity_extractor = EntityExtractor(model_name=\"your model name on huggingface\")\n",
    "entity_meta_list = entity_extractor.extract(nodes=nodes)\n",
    "for item in entity_meta_list:\n",
    "    logger.debug(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.01✅.Prepare Storage Context: Mongo```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore.mongodb import MongoDocumentStore\n",
    "from llama_index.storage.index_store.mongodb import MongoIndexStore\n",
    "from llama_index.core import StorageContext\n",
    "mongo_storage_context = StorageContext.from_defaults(\n",
    "    docstore=MongoDocumentStore.from_uri(uri=mongo_uri),\n",
    "    index_store=MongoIndexStore.from_uri(uri=mongo_uri),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.02✅. Prepare Storage Context: Postgres```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.index_store.postgres import PostgresIndexStore\n",
    "from llama_index.storage.docstore.postgres import PostgresDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "pg_storage_context = StorageContext.from_defaults(\n",
    "    index_store=PostgresIndexStore.from_uri(uri=pg_uri),\n",
    "    docstore=PostgresDocumentStore.from_uri(uri=pg_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```06.01✅.Persist Chuncks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_storage_context.docstore.add_documents(nodes)\n",
    "mongo_storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```07.01✅.create document indices and persist indices```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (DocumentSummaryIndex, \n",
    "                              KeywordTableIndex, \n",
    "                              KnowledgeGraphIndex, \n",
    "                              PropertyGraphIndex,\n",
    "                              RAKEKeywordTableIndex,\n",
    "                              SimpleKeywordTableIndex,\n",
    "                              SummaryIndex, \n",
    "                              TreeIndex, \n",
    "                              VectorStoreIndex,\n",
    "                              ListIndex, \n",
    "                              GPTListIndex,\n",
    "                              GPTVectorStoreIndex,\n",
    "                              GPTTreeIndex,\n",
    "                              GPTSimpleKeywordTableIndex,)\n",
    "\n",
    "summary_index = SummaryIndex(nodes, storage_context=pg_storage_context)\n",
    "vector_index = VectorStoreIndex(nodes=nodes, embed_model=embedding, storage_context=pg_storage_context)\n",
    "simple_keyword_index = SimpleKeywordTableIndex(nodes=nodes, storage_context=pg_storage_context)\n",
    "pg_storage_context.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```07.02✅.Create Indices By ollama.ai, and persist data in online database.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore.postgres import PostgresDocumentStore\n",
    "from llama_index.storage.kvstore.postgres import PostgresKVStore\n",
    "from llama_index.indices.managed.postgresml import PostgresMLIndex\n",
    "# conn_str = 'postgresql://llm:llm@192.168.1.3:5432/llm?sslmode=require'\n",
    "# kv_store = PostgresKVStore(connection_string=conn_str, async_connection_string=conn_str, table_name=\"pdf\",)\n",
    "# doc_store = PostgresDocumentStore(postgres_kvstore=kv_store)\n",
    "os.environ[\n",
    "    \"PGML_DATABASE_URL\"\n",
    "] = \"postgres://u_avmhinwq8sk1pgv:hktkhlft7grwzt5@437a9d42-c398-4c00-9906-c9b5fc2e7d61.gcp.db.postgresml.org:6432/pgml_7eqv4awesvjc0u9\"\n",
    "index = PostgresMLIndex.from_documents(collection_name= \"llama-index-test-1\", documents= document)\n",
    "# retriever = index.as_retriever()\n",
    "# results = retriever.retrieve(\"how many chapters in this book\")\n",
    "# logger.debug(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```08.01✅.Make query```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "resp = query_engine.query(\"please give the document summary.\")\n",
    "logger.debug(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
