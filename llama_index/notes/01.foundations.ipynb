{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama_index\n",
    "! pip install llama-parse\n",
    "! pip install llmsherpa\n",
    "! pip install llama-index-readers-pdf-marker\n",
    "! pip install llama-index-readers-llama-parse\n",
    "! pip install llama-index-readers-smart-pdf-loader\n",
    "! pip install llama-index-indices-managed-postgresml\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-index-store-mongodb\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-docstore-postgres\n",
    "! pip install llama-index-storage-docstore-mongodb\n",
    "! pip install llama-index-vector-stores-postgres\n",
    "! pip install llama-index-vector-stores-pinecone\n",
    "! pip install llama-index-vector-stores-chroma\n",
    "! pip install llama-index-llms-openai\n",
    "! pip install llama-index-llms-ollama\n",
    "! pip install llama-index-extractors-entity\n",
    "! pip install llama-index-extractors-marvin\n",
    "! pip install unstructured\n",
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.01✅.Set Application Runtime Environment```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import AppConfig\n",
    "from llama_index.core import Settings\n",
    "from env import AppConfig\n",
    "config = AppConfig()\n",
    "logger = config.logger\n",
    "# using ollama\n",
    "Settings.llm = config.llm\n",
    "Settings.embed_model = config.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.02✅.Test if local llm is working```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(config.llm.complete(\"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```02.01✅.Read Documents 1: Using SimpleDirectoryReader   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import (DocxReader, \n",
    "                                      HWPReader, \n",
    "                                      PDFReader, \n",
    "                                      EpubReader, \n",
    "                                      FlatReader, \n",
    "                                      HTMLTagReader, \n",
    "                                      ImageCaptionReader, \n",
    "                                      ImageReader, \n",
    "                                      ImageVisionLLMReader, \n",
    "                                      IPYNBReader, \n",
    "                                      MarkdownReader, \n",
    "                                      MboxReader, \n",
    "                                      PptxReader, \n",
    "                                      PandasCSVReader, \n",
    "                                      PandasExcelReader,\n",
    "                                      VideoAudioReader, \n",
    "                                      UnstructuredReader, \n",
    "                                      PyMuPDFReader, \n",
    "                                      ImageTabularChartReader, \n",
    "                                      XMLReader, \n",
    "                                      PagedCSVReader, \n",
    "                                      CSVReader, \n",
    "                                      RTFReader,)\n",
    "reader = SimpleDirectoryReader(\"./pdf_files\")\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```02.02✅.Read Documents 2: Using LlamaParse```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from llama_index.readers.smart_pdf_loader import SmartPDFLoader\n",
    "from llama_index.readers.pdf_marker import PDFMarkerReader\n",
    "from pathlib import Path\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "parses = LlamaParse(\n",
    "    api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"),  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "docs = parses.load_data(f\"{Path.cwd()}/pdf_files/2407.21290v1.pdf\")\n",
    "logger.debug(len(docs))\n",
    "\n",
    "\n",
    "# ❌\n",
    "# llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "# pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\"  # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "# pdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n",
    "# documents = pdf_loader.load_data(pdf_url)\n",
    "\n",
    "# ❌\n",
    "# path = Path(\"/Users/tju/Downloads/Books/1.pdf\")\n",
    "# reader = PDFMarkerReader()\n",
    "# document = reader.load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.01✅.Using splitter to separate documents to chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (TokenTextSplitter, \n",
    "                                          MetadataAwareTextSplitter,\n",
    "                                          SentenceSplitter, \n",
    "                                          CodeSplitter)\n",
    "splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \", include_prev_next_rel=True)\n",
    "nodes = splitter.get_nodes_from_documents(documents=docs)\n",
    "logger.debug(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.02✅.Using Parser to separate documents to chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import(SentenceWindowNodeParser, \n",
    "                                         SemanticDoubleMergingSplitterNodeParser, \n",
    "                                         SimpleFileNodeParser, \n",
    "                                         SemanticSplitterNodeParser,\n",
    "                                         NodeParser, \n",
    "                                         LlamaParseJsonNodeParser,\n",
    "                                         HTMLNodeParser, \n",
    "                                         JSONNodeParser, \n",
    "                                         SimpleNodeParser, \n",
    "                                         MarkdownNodeParser,\n",
    "                                         LangchainNodeParser, \n",
    "                                         HierarchicalNodeParser, \n",
    "                                         MarkdownElementNodeParser,\n",
    "                                         UnstructuredElementNodeParser)\n",
    "# has error message\n",
    "# parser = UnstructuredElementNodeParser()\n",
    "parser = SimpleFileNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents=docs)\n",
    "logger.debug(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.03✅.Using pipeline to seperate documents to chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.extractors.marvin import MarvinMetadataExtractor\n",
    "\n",
    "entity_extractor = EntityExtractor(model_name=\"your model name on huggingface\")\n",
    "entity_meta_list = entity_extractor.extract(nodes=nodes)\n",
    "for item in entity_meta_list:\n",
    "    logger.debug(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` 04.00. Using Extractor to refine meta_data ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    TitleExtractor, \n",
    "    SummaryExtractor, \n",
    "    KeywordExtractor,\n",
    "    PydanticProgramExtractor,\n",
    "    QuestionsAnsweredExtractor,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.01✅.Extract meta-data 1: TitleExtractor  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "keyword_extractor = TitleExtractor()\n",
    "keyword_meta_list = keyword_extractor.extract(nodes)\n",
    "for item in keyword_meta_list:\n",
    "    logger.debug(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.02✅.Extract meta-data 2: KeywordExtractor```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "title_extractor = TitleExtractor()\n",
    "title_meta_list = title_extractor.extract(nodes)\n",
    "\n",
    "keyword_extractor = KeywordExtractor()\n",
    "keyword_meta_list = keyword_extractor.extract(nodes=nodes)\n",
    "\n",
    "for item in keyword_meta_list:\n",
    "    logger.debug(item)\n",
    "\n",
    "for item in title_meta_list:\n",
    "    logger.debug(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.03✅. Read Document By Pipepine working with Extrctors to refine meta-data```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "title_extractor = TitleExtractor();\n",
    "keyword_extractor = KeywordExtractor()\n",
    "transformations = [sentence_splitter, title_extractor, keyword_extractor, config.embedding]\n",
    "pipeline = IngestionPipeline( transformations = transformations)\n",
    "nodes = pipeline.run(documents=docs, num_workers=4)\n",
    "logger.debug(f\"nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.01✅.Prepare Storage Context: Mongo```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore.mongodb import MongoDocumentStore\n",
    "from llama_index.storage.index_store.mongodb import MongoIndexStore\n",
    "from llama_index.storage.kvstore.mongodb import MongoDBKVStore\n",
    "from llama_index.core import StorageContext\n",
    "mongo_storage_context = StorageContext.from_defaults(\n",
    "    docstore=MongoDocumentStore.from_uri(uri=config.mongo_uri),\n",
    "    index_store=MongoIndexStore.from_uri(uri=config.mongo_uri),\n",
    "    vector_store=MongoDBKVStore.from_uri(uri=config.mongo_uri)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.02✅. Prepare Storage Context: Postgres```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.index_store.postgres import PostgresIndexStore\n",
    "from llama_index.storage.docstore.postgres import PostgresDocumentStore\n",
    "from llama_index.storage.kvstore.postgres import PostgresKVStore\n",
    "from llama_index.vector_stores.postgres.base import PGVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from sqlalchemy import make_url\n",
    "url = make_url(config.pg_uri)\n",
    "logger.debug(url)\n",
    "\n",
    "pg_idx_store = PostgresIndexStore.from_uri(uri=config.pg_uri, table_name='idx_store')\n",
    "pg_doc_store = PostgresDocumentStore.from_uri(uri=config.pg_uri, table_name='doc_store')\n",
    "pg_vec_store=PGVectorStore.from_params(\n",
    "        database=url.database,\n",
    "        host=url.host,\n",
    "        password=url.password,\n",
    "        port=url.port,\n",
    "        user=url.username,\n",
    "        table_name=\"vec_store\",\n",
    "        embed_dim=4096,  # openai embedding dimension\n",
    "        hnsw_kwargs={\n",
    "            \"hnsw_m\": 16,\n",
    "            \"hnsw_ef_construction\": 64,\n",
    "            \"hnsw_ef_search\": 40,\n",
    "            \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "pg_storage_context = StorageContext.from_defaults(index_store=pg_idx_store, \n",
    "                                                  docstore=pg_doc_store, \n",
    "                                                  vector_store=pg_vec_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```06.01✅.Persist Chuncks```\n",
    "* to avoid a document being stored multiple times, we need to make a query to confirm whether the document exists before we are going to persist any docs\n",
    "* if the doc has been stored, it will be necessary to delete it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_storage_context.docstore.add_documents(docs=docs)\n",
    "# mongo_storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```07.01✅.create document indices and persist indices```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (DocumentSummaryIndex, \n",
    "                              KeywordTableIndex, \n",
    "                              KnowledgeGraphIndex, \n",
    "                              PropertyGraphIndex,\n",
    "                              RAKEKeywordTableIndex,\n",
    "                              SimpleKeywordTableIndex,\n",
    "                              SummaryIndex, \n",
    "                              TreeIndex, \n",
    "                              VectorStoreIndex,\n",
    "                              ListIndex, \n",
    "                              GPTListIndex,\n",
    "                              GPTVectorStoreIndex,\n",
    "                              GPTTreeIndex,\n",
    "                              GPTSimpleKeywordTableIndex,)\n",
    "\n",
    "# summary_index = SummaryIndex(nodes, storage_context=pg_storage_context)\n",
    "vector_index = VectorStoreIndex.from_documents(documents=docs, \n",
    "                                               embed_model=config.embedding, \n",
    "                                               storage_context=pg_storage_context)\n",
    "# simple_keyword_index = SimpleKeywordTableIndex(nodes=nodes, storage_context=pg_storage_context)\n",
    "# pg_storage_context.index_store.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```07.02✅.Create Indices By ollama.ai, and persist data in online database.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore.postgres import PostgresDocumentStore\n",
    "from llama_index.storage.kvstore.postgres import PostgresKVStore\n",
    "from llama_index.indices.managed.postgresml import PostgresMLIndex\n",
    "# conn_str = 'postgresql://llm:llm@192.168.1.3:5432/llm?sslmode=require'\n",
    "# kv_store = PostgresKVStore(connection_string=conn_str, async_connection_string=conn_str, table_name=\"pdf\",)\n",
    "# doc_store = PostgresDocumentStore(postgres_kvstore=kv_store)\n",
    "os.environ[\n",
    "    \"PGML_DATABASE_URL\"\n",
    "] = \"postgres://u_avmhinwq8sk1pgv:hktkhlft7grwzt5@437a9d42-c398-4c00-9906-c9b5fc2e7d61.gcp.db.postgresml.org:6432/pgml_7eqv4awesvjc0u9\"\n",
    "index = PostgresMLIndex.from_documents(collection_name= \"llama-index-test-1\", documents= docs)\n",
    "# retriever = index.as_retriever()\n",
    "# results = retriever.retrieve(\"how many chapters in this book\")\n",
    "# logger.debug(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```08.01✅.Make query```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "# resp = query_engine.query(\"what do they argue about?\")\n",
    "# logger.debug(resp)\n",
    "resp = query_engine.query(\"what are the Results and Discussions in the given context?\")\n",
    "logger.debug(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
