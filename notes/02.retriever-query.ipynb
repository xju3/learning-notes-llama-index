{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama_index\n",
    "! pip install llama-parse\n",
    "! pip install llmsherpa\n",
    "! pip install llama-index-question-gen-guidance\n",
    "! pip install llama-index-question-gen-openai\n",
    "! pip install llama-index-question-gen-openai\n",
    "! pip install llama-index-readers-pdf-marker\n",
    "! pip install llama-index-readers-llama-parse\n",
    "! pip install llama-index-readers-smart-pdf-loader\n",
    "! pip install llama-index-indices-managed-postgresml\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-index-store-mongodb\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-docstore-postgres\n",
    "! pip install llama-index-storage-docstore-mongodb\n",
    "! pip install llama-index-vector-stores-postgres\n",
    "! pip install llama-index-vector-stores-pinecone\n",
    "! pip install llama-index-vector-stores-chroma\n",
    "! pip install llama-index-llms-openai\n",
    "! pip install llama-index-llms-ollama\n",
    "! pip install llama-index-extractors-entity\n",
    "! pip install llama-index-extractors-marvin\n",
    "! pip install unstructured\n",
    "! pip install lxml\n",
    "! pip install spacy\n",
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 Preparing the runtime enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from common.env import AppConfig\n",
    "from llama_index.core import Settings\n",
    "config = AppConfig()\n",
    "logger = config.logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. load vector index from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from sqlalchemy import make_url\n",
    "url = make_url(config.pg_uri)\n",
    "pg_vec_store = PGVectorStore.from_params(\n",
    "    database=url.database, \n",
    "    host=url.host, \n",
    "    password=url.password, \n",
    "    port=url.port, \n",
    "    user=url.username, \n",
    "    schema_name='qwen',\n",
    "    table_name=\"vec_store_3584\", \n",
    "    embed_dim=3584,  # openai embedding dimension \n",
    "    hnsw_kwargs={\n",
    "            \"hnsw_m\": 16,\n",
    "            \"hnsw_ef_construction\": 64,\n",
    "            \"hnsw_ef_search\": 40,\n",
    "            \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "        })\n",
    "pg_vec_index = VectorStoreIndex.from_vector_store(vector_store=pg_vec_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Testing The Indices retrieved from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = pg_vec_index.as_query_engine(llm=Settings.llm)\n",
    "# q = '林斌是谁'\n",
    "q = '林斌是哪年出生的'\n",
    "# q = '文档内容中提到哪家会计师事务所'\n",
    "# q = '详细介绍一下文件中的林斌'\n",
    "# q = '林斌的教育背景'\n",
    "# q = '林斌有硕士学历吗'\n",
    "# q = '林斌本科毕业于哪所大学' \n",
    "q1 = '林斌是哪年出生的且毕业于哪所大学'\n",
    "# # resp = query_engine.query(q)\n",
    "# logger.debug(resp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Query Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Question Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.1.1 define meter-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import (FilterOperator, \n",
    "                                                  FilterCondition, \n",
    "                                                  MetadataFilter, \n",
    "                                                  MetadataFilters) \n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[MetadataFilter(key=\"department\",  value=\"Procurement\"  ),\n",
    "             MetadataFilter(key=\"security_classification\",  value='',  operator=FilterOperator.LTE)],  \n",
    "    condition=FilterCondition.AND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.1.2.Define Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import (LLMMultiSelector, \n",
    "                                        PydanticMultiSelector,\n",
    "                                        LLMSingleSelector)\n",
    "options = [\n",
    "    \"option 1: this is good for summarization questions\",  \n",
    "    \"option 2: this is useful for precise definitions\",  \n",
    "    \"option 3: this is useful for comparing concepts\",]\n",
    "selector = LLMSingleSelector.from_defaults() \n",
    "selections = selector.select(options,  \n",
    "                           query=\"What's the definition of space?\"  )\n",
    "logger.debug(type(selections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import (AutoMergingRetriever, \n",
    "                                         BaseRetriever,\n",
    "                                         BaseImageRetriever, \n",
    "                                         EmptyIndexRetriever, \n",
    "                                         KeywordTableSimpleRetriever,\n",
    "                                         KGTableRetriever, \n",
    "                                         KnowledgeGraphRAGRetriever,\n",
    "                                         LLMSynonymRetriever, ListIndexRetriever,\n",
    "                                         RecursiveRetriever,\n",
    "                                         RouterRetriever,\n",
    "                                         TextToCypherRetriever,\n",
    "                                         TransformRetriever,\n",
    "                                         TreeRootRetriever,\n",
    "                                         TreeSelectLeafRetriever,\n",
    "                                         TreeSelectLeafEmbeddingRetriever,\n",
    "                                         SummaryIndexEmbeddingRetriever, \n",
    "                                         SummaryIndexLLMRetriever,\n",
    "                                         SummaryIndexRetriever,\n",
    "                                         VectorIndexRetriever,\n",
    "                                         VectorContextRetriever,\n",
    "                                         VectorIndexAutoRetriever,\n",
    "                                         )\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "\n",
    "retriever = VectorIndexRetriever(index=pg_vec_index, \n",
    "                                 vector_store_query_mode=VectorStoreQueryMode.DEFAULT, \n",
    "                                 embed_model=Settings.embed_model, \n",
    "                                 filters=[],\n",
    "                                 callback_manager=None,\n",
    "                                 alpha=0.9,\n",
    "                                 verbose=True)\n",
    "\n",
    "resp = retriever.retrieve(q)\n",
    "logger.debug(len(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool\n",
    "vector_tool = RetrieverTool.from_defaults(retriever=retriever, description=\"....\")\n",
    "\n",
    "router_retriever = RouterRetriever(selector=selector,\n",
    "                                   retriever_tools=[vector_tool], llm=Settings.llm)\n",
    "resp = router_retriever.retrieve(q)\n",
    "logger.debug(type(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.Questions Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.4.1.HyDEQueryTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import  DecomposeQueryTransform\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(q1)\n",
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.4.2.QuestionGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "query_gen_str = \"\"\"\\\n",
    "You are a helpful assistant that generates multiple search queries based on a \\\n",
    "single input query. Generate {num_queries} search queries, one on each line, \\\n",
    "related to the following input query:\n",
    "Query: {query}\n",
    "Queries:\n",
    "\"\"\"\n",
    "query_gen_prompt = PromptTemplate(query_gen_str)\n",
    "\n",
    "def generate_queries(query: str, llm, num_queries: int = 2):\n",
    "    response = llm.predict(\n",
    "        query_gen_prompt, num_queries=num_queries, query=query\n",
    "    )\n",
    "    # assume LLM proper put each query on a newline\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries\n",
    "queries = generate_queries(q1, Settings.llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.Query multiple documents with SubQuestionQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5.Postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import (AutoPrevNextNodePostprocessor, \n",
    "                                            EmbeddingRecencyPostprocessor,\n",
    "                                            FixedRecencyPostprocessor,\n",
    "                                            KeywordNodePostprocessor, #designed to refine the selection of nodes based on specific  keywords.\n",
    "                                            LongContextReorder,\n",
    "                                            MetadataReplacementPostProcessor,\n",
    "                                            NERPIINodePostprocessor,\n",
    "                                            PIINodePostprocessor, \n",
    "                                            PrevNextNodePostprocessor, # designed to enhance node retrieval by fetching additional  nodes based on their relational context in the document.\n",
    "                                            SimilarityPostprocessor,\n",
    "                                            SentenceEmbeddingOptimizer,\n",
    "                                            TimeWeightedPostprocessor,)\n",
    "original_nodes = retriever.retrieve(question)\n",
    "pp =SimilarityPostprocessor(similarity_cutoff=0.8)\n",
    "remaining_nodes = pp.postprocess_nodes(original_nodes)\n",
    "for node in remaining_nodes:\n",
    "    logger.debug(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.ReRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CohereRerank: [website](https://cohere.com/rerank)\n",
    "# LongLLMLinguaPostprocessor: [Github](https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb)\n",
    "# how to gauge the quality of the re-ranking step?\n",
    "# understand the model drift phenomenon.\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "from llama_index.core.postprocessor.sbert_rerank import SentenceTransformerRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7 Response Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.schema import TextNode, NodeWithScore\n",
    "\n",
    "nodes = [ \n",
    "    TextNode(text=\n",
    "        \"The town square clock was built in 1895\"\n",
    "    ), \n",
    "    TextNode(text=\n",
    "        \"A turquoise parrot lives in the Amazon\"\n",
    "    ), \n",
    "    TextNode(text=\n",
    "        \"A rare orchid blooms only at midnight\"\n",
    "    ), \n",
    "] \n",
    "\n",
    "node_with_score_list = [NodeWithScore(node=node) for node in nodes] \n",
    "synth = get_response_synthesizer( \n",
    "    response_mode=\"refine\", # this is a template.[ResponseMode.IMPACT]\n",
    "    use_async=False, \n",
    "    streaming=False, \n",
    ") \n",
    "\n",
    "response = synth.synthesize( \n",
    "    \"When was the clock built?\", \n",
    "    nodes=node_with_score_list \n",
    ") \n",
    "logger.debug(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.8 OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.8.1 [GuarDrailsOutputParser](http://www.guardrailsai.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.selectors import PydanticMultiSelector\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core import SummaryIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"files/sample\").load_data()\n",
    "title_extractor = TitleExtractor()\n",
    "for doc in documents:\n",
    "    title_metadata = title_extractor.extract([doc])\n",
    "    doc.metadata.update(title_metadata[0])\n",
    "\n",
    "indexes = []\n",
    "query_engines = []\n",
    "tools = []\n",
    "\n",
    "for doc in documents:\n",
    "    document_title = doc.metadata['document_title']\n",
    "    file_name = doc.metadata['file_name']\n",
    "    index = SummaryIndex.from_documents([doc])\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        name=file_name,\n",
    "        description=f\"Contains data about {document_title}\",\n",
    "    )\n",
    "    indexes.append(index)\n",
    "    query_engines.append(query_engine)\n",
    "    tools.append(tool)\n",
    "\n",
    "qe = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=tools,\n",
    "    use_async=True\n",
    ")\n",
    "\n",
    "response = qe.query(\n",
    "    \"Compare buildings from ancient Athens and ancient Rome\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0. Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Exploring different methods of building query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import (BaseQueryEngine,\n",
    "                                           ComposableGraphQueryEngine,\n",
    "                                           CogniswitchQueryEngine,\n",
    "                                           CitationQueryEngine, \n",
    "                                           CustomQueryEngine, \n",
    "                                           JSONalyzeQueryEngine, \n",
    "                                           KnowledgeGraphQueryEngine,\n",
    "                                           MultiStepQueryEngine,  \n",
    "                                           NLSQLTableQueryEngine, \n",
    "                                           PandasQueryEngine, \n",
    "                                           PGVectorSQLQueryEngine, \n",
    "                                           RetrieverQueryEngine, \n",
    "                                           RetrieverRouterQueryEngine, \n",
    "                                           RetryGuidelineQueryEngine, \n",
    "                                           RetryQueryEngine, \n",
    "                                           RouterQueryEngine, # used for seperater the query corresponding with perticular retrievers\n",
    "                                           RetrySourceQueryEngine, \n",
    "                                           SQLJoinQueryEngine, \n",
    "                                           SimpleMultiModalQueryEngine,\n",
    "                                           SQLAutoVectorQueryEngine, \n",
    "                                           SQLTableRetrieverQueryEngine,\n",
    "                                           SubQuestionQueryEngine, # devide complex question to sub question and invoke corresponding retriever. \n",
    "                                           TransformQueryEngine, \n",
    "                                           ToolRetrieverRouterQueryEngine,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.Query multiple documents with SubQuestionQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import PydanticMultiSelector \n",
    "from llama_index.core import SummaryIndex, SimpleDirectoryReader\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"files\").load_data()\n",
    "\n",
    "title_extractor = TitleExtractor()\n",
    "for doc in documents:\n",
    "    title_metadata = title_extractor.extract([doc])\n",
    "    doc.metadata.update(title_metadata[0])\n",
    "\n",
    "indexes = []\n",
    "query_engines = []\n",
    "tools = []\n",
    "\n",
    "for doc in documents:\n",
    "    document_title = doc.metadata['document_title']\n",
    "    index = SummaryIndex.from_documents([doc])\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        description=f\"Contains data about {document_title}\",\n",
    "    )\n",
    "    indexes.append(index)\n",
    "    query_engines.append(query_engine)\n",
    "    tools.append(tool)\n",
    "\n",
    "qe = RouterQueryEngine(\n",
    "    selector=PydanticMultiSelector.from_defaults(),\n",
    "    query_engine_tools=tools\n",
    ")\n",
    "\n",
    "response = qe.query(\n",
    "    \"Tell me about Rome and dogs\"\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
