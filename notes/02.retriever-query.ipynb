{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama_index\n",
    "! pip install llama-parse\n",
    "! pip install llmsherpa\n",
    "! pip install llama-index-readers-pdf-marker\n",
    "! pip install llama-index-readers-llama-parse\n",
    "! pip install llama-index-readers-smart-pdf-loader\n",
    "! pip install llama-index-indices-managed-postgresml\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-index-store-mongodb\n",
    "! pip install llama-index-storage-index-store-postgres\n",
    "! pip install llama-index-storage-docstore-postgres\n",
    "! pip install llama-index-storage-docstore-mongodb\n",
    "! pip install llama-index-vector-stores-postgres\n",
    "! pip install llama-index-vector-stores-pinecone\n",
    "! pip install llama-index-vector-stores-chroma\n",
    "! pip install llama-index-llms-openai\n",
    "! pip install llama-index-llms-ollama\n",
    "! pip install llama-index-extractors-entity\n",
    "! pip install llama-index-extractors-marvin\n",
    "! pip install unstructured\n",
    "! pip install lxml\n",
    "! pip install spacy\n",
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import AppConfig\n",
    "from llama_index.core import Settings\n",
    "config = AppConfig()\n",
    "logger = config.logger\n",
    "# resp = Settings.llm.complete(\"hello\")\n",
    "# config.logger.debug(resp)\n",
    "# question = 'who comes from Lawrence Berkely National Labatorry in this book'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.01. laod vector index from database ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from sqlalchemy import make_url\n",
    "url = make_url(config.pg_uri)\n",
    "pg_vec_store = PGVectorStore.from_params(\n",
    "    database=url.database, \n",
    "    host=url.host, \n",
    "    password=url.password, \n",
    "    port=url.port, \n",
    "    user=url.username, \n",
    "    table_name=\"vec_store\", \n",
    "    embed_dim=4096,  # openai embedding dimension \n",
    "    hnsw_kwargs={\n",
    "            \"hnsw_m\": 16,\n",
    "            \"hnsw_ef_construction\": 64,\n",
    "            \"hnsw_ef_search\": 40,\n",
    "            \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "        })\n",
    "pg_vec_index = VectorStoreIndex.from_vector_store(vector_store=pg_vec_store, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.02 define meter-filters ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import (FilterOperator, \n",
    "                                                  FilterCondition, \n",
    "                                                  MetadataFilter, \n",
    "                                                  MetadataFilters) \n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[MetadataFilter(key=\"department\",  value=\"Procurement\"  ),\n",
    "             MetadataFilter(key=\"security_classification\",  value='',  operator=FilterOperator.LTE)],  \n",
    "    condition=FilterCondition.AND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.03. Define Selectors ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import (LLMMultiSelector, \n",
    "                                        PydanticMultiSelector,\n",
    "                                        LLMSingleSelector)\n",
    "options = [\n",
    "    \"option 1: this is good for summarization questions\",  \n",
    "    \"option 2: this is useful for precise definitions\",  \n",
    "    \"option 3: this is useful for comparing concepts\",]\n",
    "selector = LLMSingleSelector.from_defaults() \n",
    "selections = selector.select(options,  \n",
    "                           query=\"What's the definition of space?\"  )\n",
    "logger.debug(type(selections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.04.Retrievers```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import (AutoMergingRetriever, \n",
    "                                         BaseRetriever,\n",
    "                                         BaseImageRetriever, \n",
    "                                         EmptyIndexRetriever, \n",
    "                                         KeywordTableSimpleRetriever,\n",
    "                                         KGTableRetriever, \n",
    "                                         KnowledgeGraphRAGRetriever,\n",
    "                                         LLMSynonymRetriever, ListIndexRetriever,\n",
    "                                         RecursiveRetriever,\n",
    "                                         RouterRetriever,\n",
    "                                         TextToCypherRetriever,\n",
    "                                         TransformRetriever,\n",
    "                                         TreeRootRetriever,\n",
    "                                         TreeSelectLeafRetriever,\n",
    "                                         TreeSelectLeafEmbeddingRetriever,\n",
    "                                         SummaryIndexEmbeddingRetriever, \n",
    "                                         SummaryIndexLLMRetriever,\n",
    "                                         SummaryIndexRetriever,\n",
    "                                         VectorIndexRetriever,\n",
    "                                         VectorContextRetriever,\n",
    "                                         VectorIndexAutoRetriever,\n",
    "                                         )\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "\n",
    "retriever = VectorIndexRetriever(index=pg_vec_index, \n",
    "                                 vector_store_query_mode=VectorStoreQueryMode.DEFAULT, \n",
    "                                 embed_model=Settings.embed_model, \n",
    "                                 filters=[],\n",
    "                                 callback_manager=None,\n",
    "                                 alpha=0.9,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.05.Define Tools ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import RetrieverTool\n",
    "vector_tool = RetrieverTool.from_defaults(retriever=retriever, description=\"....\")\n",
    "\n",
    "router_retriever = RouterRetriever(selector=selector,\n",
    "                                   retriever_tools=[vector_tool], llm=Settings.llm)\n",
    "resp = router_retriever.retrieve(question)\n",
    "logger.debug(type(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.06. DecomposeQueryTransform```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import  DecomposeQueryTransform  \n",
    "decompose = DecomposeQueryTransform(llm=Settings.llm,)  \n",
    "query_bundle = decompose.run(\"Who comes from Lawrence Berkeley National Labaratory and when did the LBNL established\") \n",
    "logger.debug(f'bundle: {query_bundle.query_str}')\n",
    "query_engine = idx.as_query_engine()\n",
    "resp = query_engine.query(query_bundle.query_str)\n",
    "logger.debug(resp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```01.07. [OpenAIQuestionGenerator]() ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```02.00 Postprocessors```\n",
    "- Node Filtering Postprocessors\n",
    "- Node Transforming Postprocessors\n",
    "- Node Re-Ranking Postprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```02.01. Postprocessors```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import (AutoPrevNextNodePostprocessor, \n",
    "                                            EmbeddingRecencyPostprocessor,\n",
    "                                            FixedRecencyPostprocessor,\n",
    "                                            KeywordNodePostprocessor, #designed to refine the selection of nodes based on specific  keywords.\n",
    "                                            LongContextReorder,\n",
    "                                            MetadataReplacementPostProcessor,\n",
    "                                            NERPIINodePostprocessor,\n",
    "                                            PIINodePostprocessor, \n",
    "                                            PrevNextNodePostprocessor, # designed to enhance node retrieval by fetching additional  nodes based on their relational context in the document.\n",
    "                                            SimilarityPostprocessor,\n",
    "                                            SentenceEmbeddingOptimizer,\n",
    "                                            TimeWeightedPostprocessor,)\n",
    "original_nodes = retriever.retrieve(question)\n",
    "pp =SimilarityPostprocessor(similarity_cutoff=0.8)\n",
    "remaining_nodes = pp.postprocess_nodes(original_nodes)\n",
    "for node in remaining_nodes:\n",
    "    logger.debug(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```02.02 ReRanking ```\n",
    "* CohereRerank: [website](https://cohere.com/rerank)\n",
    "* LongLLMLinguaPostprocessor: [Github](https://github.com/microsoft/LLMLingua/blob/main/examples/RAGLlamaIndex.ipynb)\n",
    "* how to gauge the quality of the re-ranking step?\n",
    "* understand the model drift phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
    "from llama_index.core.postprocessor.sbert_rerank import SentenceTransformerRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```03.00. Response Synthesizer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.schema import TextNode, NodeWithScore\n",
    "\n",
    "nodes = [ \n",
    "    TextNode(text=\n",
    "        \"The town square clock was built in 1895\"\n",
    "    ), \n",
    "    TextNode(text=\n",
    "        \"A turquoise parrot lives in the Amazon\"\n",
    "    ), \n",
    "    TextNode(text=\n",
    "        \"A rare orchid blooms only at midnight\"\n",
    "    ), \n",
    "] \n",
    "\n",
    "node_with_score_list = [NodeWithScore(node=node) for node in nodes] \n",
    "synth = get_response_synthesizer( \n",
    "    response_mode=\"refine\", # this is a template.[ResponseMode.IMPACT]\n",
    "    use_async=False, \n",
    "    streaming=False, \n",
    ") \n",
    "\n",
    "response = synth.synthesize( \n",
    "    \"When was the clock built?\", \n",
    "    nodes=node_with_score_list \n",
    ") \n",
    "logger.debug(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```04.00 Implement output parsing techniques```\n",
    "* [GuarDrailsOutputParser](http://www.guardrailsai.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers.langchain import LangchainOutputParser\n",
    "from llama_index.core.output_parsers.base import BaseOutputParser\n",
    "from llama_index.core.output_parsers.pydantic import PydanticOutputParser\n",
    "from llama_index.core.output_parsers.selection import SelectionOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.00. Exploring different methods of building query Engines```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import (BaseQueryEngine,\n",
    "                                           ComposableGraphQueryEngine,\n",
    "                                           CogniswitchQueryEngine,\n",
    "                                           CitationQueryEngine, \n",
    "                                           CustomQueryEngine, \n",
    "                                           JSONalyzeQueryEngine, \n",
    "                                           KnowledgeGraphQueryEngine,\n",
    "                                           MultiStepQueryEngine,  \n",
    "                                           NLSQLTableQueryEngine, \n",
    "                                           PandasQueryEngine, \n",
    "                                           PGVectorSQLQueryEngine, \n",
    "                                           RetrieverQueryEngine, \n",
    "                                           RetrieverRouterQueryEngine, \n",
    "                                           RetryGuidelineQueryEngine, \n",
    "                                           RetryQueryEngine, \n",
    "                                           RouterQueryEngine, # used for seperater the query corresponding with perticular retrievers\n",
    "                                           RetrySourceQueryEngine, \n",
    "                                           SQLJoinQueryEngine, \n",
    "                                           SimpleMultiModalQueryEngine,\n",
    "                                           SQLAutoVectorQueryEngine, \n",
    "                                           SQLTableRetrieverQueryEngine,\n",
    "                                           SubQuestionQueryEngine, # devide complex question to sub question and invoke corresponding retriever. \n",
    "                                           TransformQueryEngine, \n",
    "                                           ToolRetrieverRouterQueryEngine,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.01.implement advanced routing with RouterQueryEngine```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import PydanticMultiSelector \n",
    "from llama_index.core import SummaryIndex, SimpleDirectoryReader\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "documents = SimpleDirectoryReader(\"files\").load_data()\n",
    "\n",
    "title_extractor = TitleExtractor()\n",
    "for doc in documents:\n",
    "    title_metadata = title_extractor.extract([doc])\n",
    "    doc.metadata.update(title_metadata[0])\n",
    "\n",
    "indexes = []\n",
    "query_engines = []\n",
    "tools = []\n",
    "\n",
    "for doc in documents:\n",
    "    document_title = doc.metadata['document_title']\n",
    "    index = SummaryIndex.from_documents([doc])\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        description=f\"Contains data about {document_title}\",\n",
    "    )\n",
    "    indexes.append(index)\n",
    "    query_engines.append(query_engine)\n",
    "    tools.append(tool)\n",
    "\n",
    "qe = RouterQueryEngine(\n",
    "    selector=PydanticMultiSelector.from_defaults(),\n",
    "    query_engine_tools=tools\n",
    ")\n",
    "\n",
    "response = qe.query(\n",
    "    \"Tell me about Rome and dogs\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```05.02.Query multiple documents with SubQuestionQueryEngine```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.selectors import PydanticMultiSelector\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core import SummaryIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"files/sample\").load_data()\n",
    "title_extractor = TitleExtractor()\n",
    "for doc in documents:\n",
    "    title_metadata = title_extractor.extract([doc])\n",
    "    doc.metadata.update(title_metadata[0])\n",
    "\n",
    "indexes = []\n",
    "query_engines = []\n",
    "tools = []\n",
    "\n",
    "for doc in documents:\n",
    "    document_title = doc.metadata['document_title']\n",
    "    file_name = doc.metadata['file_name']\n",
    "    index = SummaryIndex.from_documents([doc])\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "    tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=query_engine,\n",
    "        name=file_name,\n",
    "        description=f\"Contains data about {document_title}\",\n",
    "    )\n",
    "    indexes.append(index)\n",
    "    query_engines.append(query_engine)\n",
    "    tools.append(tool)\n",
    "\n",
    "qe = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=tools,\n",
    "    use_async=True\n",
    ")\n",
    "\n",
    "response = qe.query(\n",
    "    \"Compare buildings from ancient Athens and ancient Rome\"\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
